<!-- Time-stamp: <2017-04-11 14:42:26 (slane)> -->

---
title: "Censored MLE Investigation"
author: Stephen E. Lane
date: `r format(Sys.Date(), format = '%B %d, %Y')`
output: html_document
---

## Introduction

In this document, I detail the investigation into performing a censored regression on the *blistering barnacles* (otherwise known as *biofouling*) project. This document and the ensuing analyses should be seen as exploratory, not definitive. The main reason for this document is to have notes and code/output interspersed, rather than existing as random bits of paper everywhere!

## Idea

So, the idea for this project is to test the effect of certain variables on the measured wet weight of biofouling on boats. Measurements are taken in different locations (e.g. hull, propeller).

Complicating the analysis is that the scales used for measuring the wet weight have a limit of detection of 1.5gm. That is, any measurements under 1.5gm are unreliable and are thus censored. So, to take this into account, we need to have a model that adjusts for the probability of censoring; that is, the model adjusts for the probability that a measurement will be less than 1.5gm.

```{r,setup,echo=FALSE,warning=FALSE,message=FALSE,results="hide"}
ipak <- function(pkg){
    ## Check for github packages (throw away github username)
    chk.git <- gsub(".*/", "", pkg)    
    new.pkg <- pkg[!(chk.git %in% installed.packages()[, "Package"])]
    if(!(length(new.pkg) == 0)){
        git.ind <- grep("/", new.pkg)
        if(length(git.ind) == 0){
            install.packages(new.pkg, dependencies = TRUE)
        } else {
            devtools::install_github(new.pkg[git.ind])
        }
    }
    sapply(chk.git, require, character.only = TRUE)
}
## Add github packages using gitname/reponame format
packages <- c("dplyr", "tidyr", "ggplot2", "mice")
ipak(packages)
samplesdata <- read.csv("../data/samples.csv")
## Restrict to locations of interest
sampData <- samplesdata %>% filter(LocID %in% c("HA", "PJ", "HP"))

```

The figure below shows histograms for the wet weight in each of the locations we're interested in (there are `r nrow(sampData)` observations of wet weight made on `r length(unique(sampData$boatID))` boats). The first column shows the raw values, the second shows logged values (which are approximately normal; the data have had 0.1gm added to them, so that log is defined).

```{r,echo=FALSE}
## Don't remove outliers
histData <- sampData %>%
    mutate(wwLog = log(wetWeight + 0.1)) %>%
    select(LocID, wetWeight, wwLog) %>% gather("logged", "ww", 2:3)
pl <- ggplot(histData, aes(x = ww)) + geom_histogram(bins = 11) +
    facet_grid(LocID ~ logged, scales = "free_x")
print(pl)
```

The next figure shows the same set of data, but wet weights greater than 1000gm have been removed. This makes it easier to see the distribution of the data without thinking about the outliers.

```{r,echo=FALSE}
## Remove outliers
histData2 <- histData %>%
    filter((logged == "wetWeight" & ww <= 1000) |
           (logged == "wwLog" & ww <= log(1000)))
pl2 <- ggplot(histData2, aes(x = ww)) + geom_histogram(bins = 11) +
    facet_grid(LocID ~ logged, scales = "free_x")
print(pl2)
```

### Possible model for wet weight

Hmm, this probably won't be as complicated as it sounds. A censored regression should be pretty simple to do, either via maximum likelihood, or MCMC. I think MCMC will be better in the long run though, with multiple random effects floating around.

The likelihood is pretty simple. Let $f$ be the density of wet weight with parameter vector $\theta$. If the value of wet weight is greater than 1.5gm, then

\[
L(\theta; Y) \sim f(\beta X, \sigma)
\]

and if the wet weight is below 1.5gm, then the likelihood contribution is

\[
L(\theta; Y) \sim \int_{0}^{1.5} f(\beta X, \sigma)
\]

Now, a possible choice of $f$ would be the log-normal density, as the logged values in the figures above look pretty close to normal. There are some outliers as we've already shown though. Perhaps then a more robust distribution is in order. The log-cauchy distribution is one such distribution that has heavier tails.

> I should try both the log-normal and log-cauchy distributions when fitting.

## Measurements within boats

Up to six hull quadrats were sampled on each boat, placed *haphazardly* along fore, mid-ships and aft (three port, three starboard). Each of these had a 0.5m^2^ scraping taken, so the amount of biofouling within hull quadrats can be considered consistent. Unfortunately, the labelling of each quadrat in the dataset is insufficient, and so further analysis at the quadrat level is not possible.

> Thus, measurements within quadrats will need to be considered as a random sample from some distribution (perhaps $t$-distribution to be robust?).

Samples were taken from other external surfaces of the boat, and some easily accesible internal surfaces. We also include the rudder surface and fixed keel in this analysis. All other surfaces were too inconsistently measured, and so were not considered.

## Missing data

There is missing data in the $X$ variables, which unfortunately means that the model above cannot be fit to the full data set as it currently is. The missing data is at the second level, that is, variables relating to the boat itself are missing. The table below shows the missingness patterns in the boat-level data.

```{r, missingness, echo = FALSE}
boatData <- sampData %>%
    select(-boatType, -boatCode, -paintRating, -Location, -LocCode, -samLab,
           -LocID, -wetWeight, -wetWeight1, -wetWeight2) %>% distinct() %>%
    select(-boatID) %>%
    mutate(paintType = as.integer(factor(paintType)))
miss <- md.pattern(boatData)
print(miss)

## Now, just check what amount would be missing in observation level.
ccData <- sampData %>%
    select(-boatType, -boatCode, -paintRating, -Location, -LocCode, -samLab,
           -LocID, -wetWeight, -wetWeight1, -wetWeight2) %>% na.omit()

```

We can see that there are `r rownames(miss)[1]` observations with complete boat-level data; `r paste0(rownames(miss)[2:4], collapse = ",")` missing data on `days1`, `midTrips` and `paintType` respectively; and `r paste0(rownames(miss)[5:6], collapse = ",")` missing data on both (`days1` and `midTrips`), and (`days1`, `paintType` and `midTrips`) respectively.

A complete-case data analysis would only utilise `r as.integer(rownames(miss)[1])/nrow(boatData)*100`% of the boat-level data. At the first level, the number of observations available for a complete-case analysis is `r nrow(ccData)` (`r nrow(ccData)/nrow(sampData)*100`%).


## Imputation

Multiple imputation is a possible way to deal with the missing data. There doesn't appear to be any obvious pattern to the missingness (shown above), so an iterative approach will probably be sufficient.

Because all missing variables are at the boat level, it would make sense to do the imputation at the boat level. Of course, an immediately apparent issue is that we have multiple measurements of wet weight within each boat, so how do we deal with that? The outcome needs to be included in any imputations, so that all relationships are explored. A simple approach would be to use a summary measure, such as the mean/median wet weight within each boat. A further complication is the nesting of locations within boats (as described above), but we will ignore this for imputation.

I'll try using the median (given the skewed nature of the data). The imputation will work something like the following:

0. Set all censored data as 1.5gm, and calculate the median wet weight within each boat. This aggregation should be done over all locations sampled
1. Create prediction models for each variable with missing data
     - Start with the least missing and work up, so `days1`, `paintType` and `midTrips`
     - This part can be done using `MICE`; categorical variables just need to be changed to factors for this to work
2. Create a predicted observation for each missing value in `days1`, `paintType` and `midTrips`
3. Fit the censored regression model:
    i) Use the imputed values in the censored regression model
    ii) Predicted the censored values --- this prediction shouldn't just be a vanilla prediction though. It should use the full joint distribution, i.e. a draw from the distribution, so that all variance is accounted for. See my note in Shah et al. (2014)
    iii) Take the median within boat, and return to (1)
	
	> Note: because I want to do some variable selection, it may be good to generate a single multiply-imputed dataset. To do this, in the censored regression we could include all interactions that may be of interest. Then, when it comes to model fitting, use whatever form required on that single multiply-imputed dataset.
	
4. Repeat the cycle, $r$ times ($r=20$ should suffice)
5. Draw a final imputation for the missing $X$ variables. This is one imputation.
6. Repeat the whole procedure $m$ times.

> I wonder whether I need to fit the outcome model in the imputation iterations? This could be a good little side project: in a censored regression, do we need predictions for the censored variables to make imputation work 'well'?

### Goodness of imputation

I'll need to check out the *goodness* of imputation. I can do this by plotting boxplots of the observed data, alongside the imputed data. The distributions should remain similar.

The article by Bondarenko (2016) may be useful for graphical checking of imputations.

## Where to from here?

I think that an appropriate place to go from here, is to do the simulation study (prior to the actual data fit).

How would this look?

- Generate $X$ data
- Generate $Y$ data from a regression model
- Generate missing $X$ data, that depends on $Y$
    - I probably need to look at White, Carlin or Lee for some tips here
    - The Lee and Carlin (2010) paper doesn't explain in great deal the simulations, and many were based off a so-called *synthetic* dataset
    - Another possibility is Shah et al. (2014)
- Censor the $Y$ values
    - Do a simple cutoff, i.e. if less than $c$, put $c$
- This should mean that the imputation model depends on having a good model for $Y$
    - We need to come up with ways of modelling this
    - First thing to try is just use the censored value
    - Then, probably a midpoint of the censored value and a lower bound (if relevant)
    - Then, use the actual regression form that generated the data
        - This one should work best
    - If we don't know the actual data though, what's a reasonable alternative?
        - I.e. we need some nonparametric alternative

## Simulation 1

Let's start simple. Our first goal will be to investigate imputation in a cross-sectional model. Let's call our *main* model, the substantive model; this is the model that predicts $Y$.

In the first case, the substantive model should be linear in the predictors, and then we'll test a model that includes product terms.

We will test the following specifications to impute $Y$ (for use in the $X$ imputations):

- Fill-in imputations
    - The censoring value, $c$
    - The midpoint, $c/2$
- An over-specified regression model
    - Include two-way product terms
- A correctly-specified regression model
    - i.e. in the form of the substantive model
	
In all cases, the final analysis model will use the true substantive model form for fitting.

The correctly-specified model should perform the best, but it will obviously be more computationally demanding. This should really be nothing terribly novel.

## Simulation 2

This simulation will involve a multilevel dataset. This is where the imputation model is likely to be more influential? We'll follow the same general recipe as above, creating a linear substantive model, and then a substantive model that includes product terms.

Again, we will test the following specifications to impute $Y$ (for use in the $X$ imputations):

- Fill-in imputations
    - The censoring value, $c$
    - The midpoint, $c/2$
- An over-specified regression model
    - Include two-way product terms
- A correctly-specified regression model
    - i.e. in the form of the substantive model

In all cases, the final analysis model will use the true substantive model form for fitting.
